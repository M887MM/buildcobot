# import os
# import re
# import logging
# from collections import defaultdict
# from dotenv import load_dotenv
# from openai import OpenAI
# from db import Session, Flats as DBFlats

# from langdetect import detect, LangDetectException
# from deep_translator import GoogleTranslator
# from urllib.parse import urlsplit, urlunsplit, quote

# # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
# load_dotenv()
# openai = OpenAI(api_key=os.getenv("API_KEY"))
# logger = logging.getLogger(__name__)

# # === –•—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–∞–Ω–Ω—ã—Ö ===
# user_conversations = defaultdict(list)
# last_flats_cache = {}
# last_filters_cache = {}
# shown_flats_cache = defaultdict(set)
# selected_flat_cache = {}

# # === –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —è–∑—ã–∫–∏ ===
# SUPPORTED_LANGS = {"ru", "uz", "en"}


# # === –£—Ç–∏–ª–∏—Ç–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ URL ===
# def normalize_url(url: str) -> str:
#     try:
#         parts = urlsplit(url)
#         if not parts.scheme:
#             return url
#         path = quote(parts.path, safe="/%")
#         query = quote(parts.query, safe="=&?")
#         return urlunsplit((parts.scheme, parts.netloc, path, query, parts.fragment))
#     except Exception:
#         return url


# # === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ ===
# def detect_language(text: str) -> str:
#     if not text.strip():
#         return "ru"

#     prompt = f"""
#     Detect the language of the following text and respond ONLY with one of these ISO codes:
#     - ru (Russian)
#     - en (English)
#     - uz (Uzbek)
#     - kk (Kazakh)
#     If language is not one of them, respond with "unsupported".

#     Text: "{text}"
#     """

#     try:
#         response = openai.chat.completions.create(
#             model="gpt-5-nano",
#             messages=[{"role": "user", "content": prompt}],
#         )
#         lang_code = response.choices[0].message.content.strip().lower()
#         if lang_code not in {"ru", "en", "uz", "kk"}:
#             return "unsupported"
#         return lang_code
#     except Exception as e:
#         print(f"–û—à–∏–±–∫–∞ GPT –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —è–∑—ã–∫–∞: {e}")
#         return "ru"


# # === –ü–µ—Ä–µ–≤–æ–¥ —Ç–µ–∫—Å—Ç–∞ ===
# def translate_text_if_needed(text: str, target_lang: str) -> str:
#     if not text or target_lang not in SUPPORTED_LANGS or target_lang == "ru":
#         return text

#     # if protected_words is None:
#     #     protected_words = []

#     # # –ü–æ–º–µ—á–∞–µ–º –∑–∞—â–∏—â–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–æ–º
#     # placeholder_map = {word: f"[[[{i}]]]" for i, word in enumerate(protected_words)}
#     # for word, placeholder in placeholder_map.items():
#     #     text = re.sub(re.escape(word), placeholder, text, flags=re.IGNORECASE)

#     system_prompt = f"""

# –¢—ã –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ {target_lang}.
# –ù–µ –ø–µ—Ä–µ–≤–æ–¥–∏ –∏ –Ω–µ –º–µ–Ω—è–π –Ω–∞–≤–∑–≤–∞–Ω–∏–∏ –ñ–ö –ö–æ–º–ø–∞–Ω–∏–∏ –∏ —Ç–¥.
# –ü–µ—Ä–µ–≤–æ–¥–∏ –æ—Å—Ç–∞–ª—å–Ω–æ–π —Ç–µ–∫—Å—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —Ç–æ—á–Ω–æ –∏ –∫—Ä–∞—Ç–∫–æ.
# """

#     try:
#         response = client.chat.completions.create(
#             model="gpt-5-nano",
#             messages=[
#                 {"role": "system", "content": system_prompt},
#                 {"role": "user", "content": text}
#             ]
#         )

#         translated = response.choices[0].message.content.strip()

#         # # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞—â–∏—â—ë–Ω–Ω—ã–µ —Å–ª–æ–≤–∞
#         # for word, placeholder in placeholder_map.items():
#         #     translated = translated.replace(placeholder, word)

#         return translated

#     except Exception as e:
#         print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —á–µ—Ä–µ–∑ GPT: {e}")
#         return text

# # === –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∏–ª—å—Ç—Ä–æ–≤ ===
# def parse_filters_from_text(text: str) -> dict:
#     filters = {}
#     if not text:
#         return filters
#     t = text.lower()

#     # === –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç ===
#     if m := re.search(r'(\d+)\s*(?:–∫–æ–º–Ω|–∫–æ–º–Ω–∞—Ç|xonali|room|otaq|xonasi)', t):
#         filters["rooms"] = int(m.group(1))
#     elif any(x in t for x in ["–æ–¥–Ω", "bir", "one"]):
#         filters["rooms"] = 1
#     elif any(x in t for x in ["–¥–≤—É—Ö", "ikki", "two", "2 "]):
#         filters["rooms"] = 2
#     elif any(x in t for x in ["—Ç—Ä–µ—Ö", "uch", "three", "3 "]):
#         filters["rooms"] = 3

#     # === –≠—Ç–∞–∂ ===
#     if m := re.search(r'(\d+)\s*(?:—ç—Ç–∞–∂|qavat|floor)', t):
#         filters["stage"] = int(m.group(1))

#     # === –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ ===
#     if m := re.search(r'(?:–¥–æ|gacha|up to)\s*([$]?\s*[0-9\s\.,k–∫K]+)', t):
#         s = m.group(1)
#         if s:
#             s = s.replace(' ', '').replace(',', '').replace('$', '')
#             if re.search(r'[k–∫K]', s):
#                 s = re.sub(r'[^\d]', '', s)
#                 if s:
#                     filters["price_max"] = int(s) * 1000
#             else:
#                 s = re.sub(r'[^\d]', '', s)
#                 if s:
#                     filters["price_max"] = int(s)

#     # === –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ü–µ–Ω–µ ===
#     if any(k in t for k in ["–¥–µ—à–µ–≤", "arzon", "cheap"]):
#         filters["price_order"] = "min"
#     elif any(k in t for k in ["–¥–æ—Ä–æ–≥", "qimmat", "expensive"]):
#         filters["price_order"] = "max"

#     # === –¢–∏–ø –æ–±—ä–µ–∫—Ç–∞: –∫–≤–∞—Ä—Ç–∏—Ä–∞, —Å—Ç—É–¥–∏—è, –º–∞–≥–∞–∑–∏–Ω ===
#     if any(k in t for k in ["–∫–≤–∞—Ä—Ç–∏—Ä–∞", "apartment", "uy"]):
#         filters["type"] = "–∫–≤–∞—Ä—Ç–∏—Ä–∞"
#     elif any(k in t for k in ["—Å—Ç—É–¥–∏—è", "studio"]):
#         filters["type"] = "—Å—Ç—É–¥–∏—è"
#     elif any(k in t for k in ["–º–∞–≥–∞–∑–∏–Ω", "shop", "do‚Äòkon"]):
#         filters["type"] = "–º–∞–≥–∞–∑–∏–Ω"

#     return filters


# # === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ ===
# def ask_openai_sync(user_id: int, text: str):
#     lang = detect_language(text)

#     if lang not in SUPPORTED_LANGS:
#         msg = (
#             "‚ùó –ò–∑–≤–∏–Ω–∏—Ç–µ, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —É–∑–±–µ–∫—Å–∫–∏–π, —Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫–∏.\n\n"
#             "‚ùó Sorry, only Uzbek, Russian and English languages are supported.\n\n"
#             "‚ùó Iltimos, bot faqat o‚Äòzbek, rus va ingliz tillarida ishlaydi."
#         )
#         user_conversations[user_id].append({"role": "assistant", "content": msg})
#         return {"text": msg}

#     text_lower = text.lower().strip()
#     user_conversations[user_id].append({"role": "user", "content": text})

#     filters = parse_filters_from_text(text_lower)

#     if m := re.match(r'/photo_(\d+)', text_lower):
#         flat_id = int(m.group(1))
#         res = get_flat_image_text(flat_id, lang)
#         user_conversations[user_id].append({"role": "assistant", "content": res["text"]})
#         return res

#     if not filters and not any(w in text_lower for w in ["–∫–≤–∞—Ä—Ç–∏—Ä", "flat", "uy", "room", "xonali", "–∏—â–∏", "–Ω–∞–π–¥–∏", "show"]):
#         msg = {
#             "ru": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –ø–æ–∂–µ–ª–∞–Ω–∏–µ üí¨\n–ù–∞–ø—Ä–∏–º–µ—Ä: ¬´2 –∫–æ–º–Ω–∞—Ç—ã –¥–æ 60 000 $¬ª –∏–ª–∏ ¬´–Ω–∞ —Å—Ä–µ–¥–Ω–µ–º —ç—Ç–∞–∂–µ¬ª.",
#             "uz": "Iltimos, kamida bitta talabni kiriting üí¨\nMasalan: ¬´2 xonali, narxi 60 000 $gacha¬ª yoki ¬´o‚Äòrta qavat¬ª.",
#             "en": "Please specify at least one preference üí¨\nFor example: '2 rooms up to $60,000' or 'middle floor'."
#         }[lang]
#         user_conversations[user_id].append({"role": "assistant", "content": msg})
#         return {"text": msg}

#     if filters:
#         last_filters_cache[user_id] = filters
#         shown_flats_cache[user_id].clear()
#         res = _get_flats_from_db(user_id, filters, lang)
#         _save_bot_messages(user_id, res)
#         return res

#     if any(w in text_lower for w in ["–µ—â—ë", "–µ—â–µ", "yana", "more"]):
#         filters = last_filters_cache.get(user_id, {})
#         if not filters:
#             msg = translate_text_if_needed("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –ø–æ–∂–µ–ª–∞–Ω–∏–µ üí¨", lang)
#             user_conversations[user_id].append({"role": "assistant", "content": msg})
#             return {"text": msg}
#         res = _get_flats_from_db(user_id, filters, lang, skip_seen=True)
#         _save_bot_messages(user_id, res)
#         return res

#     res = _get_flats_from_db(user_id, last_filters_cache.get(user_id, {}), lang)
#     _save_bot_messages(user_id, res)
#     return res


# # === –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–≤–µ—Ç—ã GPT ===
# def _save_bot_messages(user_id: int, res: dict):
#     if "text" in res:
#         user_conversations[user_id].append({"role": "assistant", "content": res["text"]})
#     if "flats" in res:
#         for f in res["flats"]:
#             user_conversations[user_id].append({"role": "assistant", "content": f["text"]})


# # === –ü–æ–∏—Å–∫ –∫–≤–∞—Ä—Ç–∏—Ä ===
# def _get_flats_from_db(user_id: int, filters: dict, lang: str, skip_seen: bool = False):
#     session = Session()
#     query = session.query(DBFlats)

#     if filters.get("rooms"):
#         query = query.filter(DBFlats.rooms == filters["rooms"])
#     if filters.get("stage"):
#         query = query.filter(DBFlats.stage == filters["stage"])
#     if filters.get("price_max"):
#         query = query.filter(DBFlats.price <= filters["price_max"])
#     if filters.get("price_order") == "min":
#         query = query.order_by(DBFlats.price.asc())
#     elif filters.get("price_order") == "max":
#         query = query.order_by(DBFlats.price.desc())

#     flats = query.all()
#     session.close()

#     if not flats:
#         msg = {
#             "ru": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –∫–≤–∞—Ä—Ç–∏—Ä—ã —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. üèô",
#             "uz": "Afsuski, bunday parametrli kvartiralar topilmadi. üèô",
#             "en": "Unfortunately, no apartments match these parameters. üèô"
#         }[lang]
#         return {"text": msg}

#     seen = shown_flats_cache[user_id]
#     new_flats = [f for f in flats if f.number not in seen][:4]
#     if not new_flats:
#         shown_flats_cache[user_id].clear()
#         new_flats = flats[:4]

#     for f in new_flats:
#         seen.add(f.number)

#     results = []
#     for f in new_flats:
#         base_text = (
#             f"üë§ {f.rooms} –∫–æ–º–Ω–∞—Ç—ã\n"
#             f"üè† –ö–≤–∞—Ä—Ç–∏—Ä–∞ ‚Ññ{f.number}\n\n"
#             f"üè† –ñ–ö Royal Residence\n"
#             f"‚Ä¢ –ö–æ–º–Ω–∞—Ç: {f.rooms}\n"
#             f"‚Ä¢ –≠—Ç–∞–∂: {f.stage}\n"
#             f"‚Ä¢ –ü–ª–æ—â–∞–¥—å: {f.sq_m} –º¬≤\n"
#             f"‚Ä¢ –¶–µ–Ω–∞: {f.price} $\n"
#             f"‚Ä¢ –ü–æ–¥—ä–µ–∑–¥: {f.lobby}\n\n"
#             "–° –≤–∞–º–∏ —Å–≤—è–∂–µ—Ç—Å—è –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π. üèô"
#         )
#         user_text = translate_text_if_needed(base_text, lang)
#         photo_val = normalize_url(f.plan.strip()) if getattr(f, "plan", None) else None
#         results.append({"text": user_text, "photo": photo_val})

#     return {"flats": results}


# # === –§–æ—Ç–æ –∫–≤–∞—Ä—Ç–∏—Ä—ã ===
# def get_flat_image_text(flat_id: int, lang: str):
#     session = Session()
#     flat = session.query(DBFlats).filter(DBFlats.number == flat_id).first()
#     session.close()

#     if not flat:
#         msg = {
#             "ru": f"–ö–≤–∞—Ä—Ç–∏—Ä–∞ —Å –Ω–æ–º–µ—Ä–æ–º {flat_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.",
#             "uz": f"{flat_id}-raqamli kvartira topilmadi.",
#             "en": f"Apartment #{flat_id} not found."
#         }[lang]
#         return {"text": msg}

#     base_text = (
#         f"üë§ {flat.rooms} –∫–æ–º–Ω–∞—Ç—ã\n"
#         f"üè† –ö–≤–∞—Ä—Ç–∏—Ä–∞ ‚Ññ{flat.number}\n\n"
#         f"üè† –ñ–ö Royal Residence\n"
#         f"‚Ä¢ –ö–æ–º–Ω–∞—Ç: {flat.rooms}\n"
#         f"‚Ä¢ –≠—Ç–∞–∂: {flat.stage}\n"
#         f"‚Ä¢ –ü–ª–æ—â–∞–¥—å: {flat.sq_m} –º¬≤\n"
#         f"‚Ä¢ –¶–µ–Ω–∞: {flat.price} $\n"
#         f"‚Ä¢ –ü–æ–¥—ä–µ–∑–¥: {flat.lobby}\n\n"
#         "–° –≤–∞–º–∏ —Å–≤—è–∂–µ—Ç—Å—è –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π. üèô"
#     )

#     user_text = translate_text_if_needed(base_text, lang)
#     photo_val = normalize_url(flat.plan.strip()) if flat.plan else None
#     return {"text": user_text, "photo": photo_val}


# # === –§–æ—Ä–º–∞—Ç –¥–∏–∞–ª–æ–≥–∞ ===
# def get_formatted_dialog(user_id: int) -> str:
#     msgs = user_conversations.get(user_id, [])
#     lines = []
#     for m in msgs:
#         prefix = "üë§" if m["role"] == "user" else "ü§ñ"
#         lines.append(f"{prefix} {m['content'].strip()}")
#     return "\n".join(lines) if lines else "‚Äî"


# # === –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ ===
# def clear_user_conversation(user_id: int):
#     user_conversations[user_id].clear()
#     last_flats_cache.pop(user_id, None)
#     last_filters_cache.pop(user_id, None)
#     shown_flats_cache.pop(user_id, None)
#     selected_flat_cache.pop(user_id, None)



# 
# import os
# import re
# import logging
# from collections import defaultdict
# from dotenv import load_dotenv
# from openai import OpenAI
# from db import Session, Flats as DBFlats
# from urllib.parse import urlsplit, urlunsplit, quote
# import json

# # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
# load_dotenv()
# client = OpenAI(api_key=os.getenv("API_KEY"))
# logger = logging.getLogger(__name__)

# # === –ö—ç—à–∏ ===
# user_conversations = defaultdict(list)
# last_filters_cache = {}
# shown_flats_cache = defaultdict(set)

# SUPPORTED_LANGS = {"ru", "uz", "en", "kk"}


# def normalize_url(url: str) -> str:
#     try:
#         parts = urlsplit(url)
#         path = quote(parts.path, safe="/%") if parts.path else ""
#         query = quote(parts.query, safe="=&?") if parts.query else ""
#         return urlunsplit((parts.scheme, parts.netloc, path, query, parts.fragment))
#     except Exception:
#         return url


# def get_formatted_dialog(user_id: int) -> str:
#     msgs = user_conversations.get(user_id, [])
#     if not msgs:
#         return "‚Äî"
#     lines = []
#     for m in msgs:
#         prefix = "üë§" if m["role"] == "user" else "ü§ñ"
#         lines.append(f"{prefix} {m['content'].strip()}")
#     return "\n".join(lines)


# def ask_openai_sync(user_id: int, text: str):
#     """
#     –ù–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏:
#     1. GPT —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç —è–∑—ã–∫, –ø–∞—Ä—Å–∏—Ç —Ñ–∏–ª—å—Ç—Ä—ã –∏ –¥–µ–ª–∞–µ—Ç —à–∞–±–ª–æ–Ω –æ—Ç–≤–µ—Ç–∞.
#     2. SQLAlchemy –±–µ—Ä—ë—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –∏–∑ –±–∞–∑—ã –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
#     """

#     text = text.strip()
#     if not text:
#         return {"text": "‚ùó –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"}

#     user_conversations[user_id].append({"role": "user", "content": text})

#     # --- –®–∞–≥ 1: GPT –ø–∞—Ä—Å–µ—Ä + –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫ ---
#     try:
#         gpt_prompt = f"""
#         –¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏. 
#         –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–∏—à–µ—Ç —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞. 
#         –¢–≤–æ—è –∑–∞–¥–∞—á–∞:
#         1. –û–ø—Ä–µ–¥–µ–ª–∏ —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–∞ (ru, en, uz, kk) –∏ –≤–µ—Ä–Ω–∏ –∫–∞–∫ "lang".
#         2. –û–ø—Ä–µ–¥–µ–ª–∏ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–≤–∞—Ä—Ç–∏—Ä/—Å—Ç—É–¥–∏–π/–º–∞–≥–∞–∑–∏–Ω–æ–≤:
#            - type: "–ö–≤–∞—Ä—Ç–∏—Ä–∞", "–°—Ç—É–¥–∏—è", "–ú–∞–≥–∞–∑–∏–Ω" –∏–ª–∏ –ø—É—Å—Ç–æ
#            - rooms: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
#            - stage: —ç—Ç–∞–∂, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
#            - price_max: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞, –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
#            - price_order: "min" –∏–ª–∏ "max" –µ—Å–ª–∏ –µ—Å—Ç—å —Å–ª–æ–≤–∞ –¥–µ—à–µ–≤–æ/–¥–æ—Ä–æ–≥–æ
#         3. –í–µ—Ä–Ω–∏ —à–∞–±–ª–æ–Ω —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –Ω–∞ —Ç–æ–º —è–∑—ã–∫–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –±—ã–ª –∑–∞–ø—Ä–æ—Å. 
#            –ù–µ –ø–µ—Ä–µ–≤–æ–¥–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –ñ–ö.
#         –§–æ—Ä–º–∞—Ç JSON:
#         {{
#             "filters": {{
#                 "type": "...",
#                 "rooms": ...,
#                 "stage": ...,
#                 "price_max": ...,
#                 "price_order": "..."
#             }},
#             "lang": "...",
#             "template_text": "..."
#         }}
#         –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞: "{text}"
#         """
#         response = client.chat.completions.create(
#             model="gpt-5-nano",
#             messages=[{"role": "user", "content": gpt_prompt}],
#         )
#         gpt_output = response.choices[0].message.content.strip()

#         # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
#         try:
#             gpt_data = json.loads(gpt_output)
#         except Exception:
#             logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç GPT: {gpt_output}")
#             gpt_data = {
#                 "filters": {},
#                 "lang": "ru",
#                 "template_text": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞."
#             }

#         filters = gpt_data.get("filters", {})
#         lang = gpt_data.get("lang", "ru")
#         template_text = gpt_data.get("template_text", "")
#     except Exception as e:
#         logger.error(f"–û—à–∏–±–∫–∞ GPT: {e}")
#         filters = {}
#         lang = "ru"
#         template_text = "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞."

#     if not filters:
#         user_conversations[user_id].append({"role": "assistant", "content": template_text})
#         return {"text": template_text}

#     # --- –®–∞–≥ 2: SQLAlchemy –≤—ã–±–æ—Ä–∫–∞ ---
#     session = Session()
#     query = session.query(DBFlats)

#     if filters.get("type"):
#         query = query.filter(DBFlats.type == filters["type"])
#     if filters.get("rooms"):
#         query = query.filter(DBFlats.rooms == filters["rooms"])
#     if filters.get("stage"):
#         query = query.filter(DBFlats.stage == filters["stage"])
#     if filters.get("price_max"):
#         query = query.filter(DBFlats.price <= filters["price_max"])

#     if filters.get("price_order") == "min":
#         query = query.order_by(DBFlats.price.asc())
#     elif filters.get("price_order") == "max":
#         query = query.order_by(DBFlats.price.desc())

#     flats = query.all()
#     session.close()

#     if not flats:
#         msg = {
#             "ru": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ–±—ä–µ–∫—Ç—ã —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. üèô",
#             "uz": "Afsuski, bunday parametrli obyektlar topilmadi. üèô",
#             "en": "Unfortunately, no properties match these parameters. üèô",
#             "kk": "”®–∫—ñ–Ω—ñ—à–∫–µ –æ—Ä–∞–π, –º“±–Ω–¥–∞–π –ø–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä–≥–µ —Å–∞–π –Ω—ã—Å–∞–Ω–¥–∞—Ä —Ç–∞–±—ã–ª–º–∞–¥—ã. üèô"
#         }.get(lang, "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ–±—ä–µ–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
#         user_conversations[user_id].append({"role": "assistant", "content": msg})
#         return {"text": msg}

#     seen = shown_flats_cache[user_id]
#     new_flats = [f for f in flats if f.number not in seen][:4]
#     if not new_flats:
#         seen.clear()
#         new_flats = flats[:4]
#     for f in new_flats:
#         seen.add(f.number)

#     # --- –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ---
#     results = []
#     for f in new_flats:
#         text_base = (
#             f"üè† {f.type} ‚Ññ{f.number}\n"
#             f"‚Ä¢ –ö–æ–º–Ω–∞—Ç: {f.rooms}\n"
#             f"‚Ä¢ –≠—Ç–∞–∂: {f.stage}\n"
#             f"‚Ä¢ –ü–ª–æ—â–∞–¥—å: {f.sq_m} –º¬≤\n"
#             f"‚Ä¢ –¶–µ–Ω–∞: {f.price} $\n"
#             f"‚Ä¢ –ü–æ–¥—ä–µ–∑–¥: {f.lobby}\n"
#             "–° –≤–∞–º–∏ —Å–≤—è–∂–µ—Ç—Å—è –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π. üèô"
#         )
#         photo_val = normalize_url(f.plan.strip()) if getattr(f, "plan", None) else None
#         results.append({"text": text_base, "photo": photo_val})
#         user_conversations[user_id].append({"role": "assistant", "content": text_base})

#     # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
#     last_filters_cache[user_id] = filters

#     return {"flats": results}


# # --- –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ ---
# def clear_user(user_id: int):
#     user_conversations[user_id].clear()
#     last_filters_cache.pop(user_id, None)
#     shown_flats_cache.pop(user_id, None)


# import os
# import logging
# from collections import defaultdict
# from dotenv import load_dotenv
# from openai import OpenAI
# from db import Session, Flats as DBFlats
# from urllib.parse import urlsplit, urlunsplit, quote

# # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
# load_dotenv()
# client = OpenAI(api_key=os.getenv("API_KEY"))
# logger = logging.getLogger(__name__)

# # === –ö—ç—à–∏ ===
# user_conversations = defaultdict(list)
# last_filters_cache = {}
# shown_flats_cache = defaultdict(set)

# SUPPORTED_LANGS = {"ru", "uz", "en", "kk"}


# def normalize_url(url: str) -> str:
#     try:
#         parts = urlsplit(url)
#         path = quote(parts.path, safe="/%") if parts.path else ""
#         query = quote(parts.query, safe="=&?") if parts.query else ""
#         return urlunsplit((parts.scheme, parts.netloc, path, query, parts.fragment))
#     except Exception:
#         return url


# def get_formatted_dialog(user_id: int) -> str:
#     msgs = user_conversations.get(user_id, [])
#     if not msgs:
#         return "‚Äî"
#     lines = []
#     for m in msgs:
#         prefix = "üë§" if m["role"] == "user" else "ü§ñ"
#         lines.append(f"{prefix} {m['content'].strip()}")
#     return "\n".join(lines)


# def detect_lang(text: str) -> str:
#     """–û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–º–æ—â—å—é GPT, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º 'ru', 'uz', 'en' –∏–ª–∏ 'kk'."""
#     try:
#         gpt_prompt = f"""
# –û–ø—Ä–µ–¥–µ–ª–∏ —è–∑—ã–∫ —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: "{text}"
# –í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: "ru", "uz", "en", "kk".
# –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ –∫–æ–¥ —è–∑—ã–∫–∞, –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤.
# """
#         response = client.chat.completions.create(
#             model="gpt-5-nano",
#             messages=[{"role": "user", "content": gpt_prompt}],
#         )
#         lang = response.choices[0].message.content.strip().lower()
#         if lang not in SUPPORTED_LANGS:
#             lang = "ru"
#         return lang
#     except Exception as e:
#         logger.error(f"–û—à–∏–±–∫–∞ GPT –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —è–∑—ã–∫–∞: {e}")
#         return "ru"


# def ask_openai_sync(user_id: int, text: str):
#     """
#     –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è:
#     1. GPT –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫.
#     2. SQLAlchemy –≤—ã–±–∏—Ä–∞–µ—Ç –æ–±—ä–µ–∫—Ç—ã –∏–∑ –±–∞–∑—ã.
#     3. –§–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –µ–≥–æ —è–∑—ã–∫–µ.
#     """
#     text = text.strip()
#     if not text:
#         return {"text": "‚ùó –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"}

#     user_conversations[user_id].append({"role": "user", "content": text})

#     # --- –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–∑—ã–∫ ---
#     lang = detect_lang(text)

#     # --- SQLAlchemy –≤—ã–±–æ—Ä–∫–∞ ---
#     session = Session()
#     query = session.query(DBFlats)

#     # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∑–¥–µ—Å—å (type, rooms, price –∏ —Ç.–¥.)
#     # –ù–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä: –∏—â–µ–º –∫–≤–∞—Ä—Ç–∏—Ä—ã –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
#     if "2 —Ö–æ–Ω–∞–ª–∏" in text or "2 –∫–æ–º–Ω–∞—Ç" in text:
#         query = query.filter(DBFlats.rooms == 2)
#     if "–º–∞–Ω–≥–∞" in text or "–∫–≤–∞—Ä—Ç–∏—Ä–∞" in text:
#         query = query.filter(DBFlats.type == "–ö–≤–∞—Ä—Ç–∏—Ä–∞")

#     flats = query.all()
#     session.close()

#     if not flats:
#         msg = {
#             "ru": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ–±—ä–µ–∫—Ç—ã —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. üèô",
#             "uz": "Afsuski, bunday parametrli obyektlar topilmadi. üèô",
#             "en": "Unfortunately, no properties match these parameters. üèô",
#             "kk": "”®–∫—ñ–Ω—ñ—à–∫–µ –æ—Ä–∞–π, –º“±–Ω–¥–∞–π –ø–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä–≥–µ —Å–∞–π –Ω—ã—Å–∞–Ω–¥–∞—Ä —Ç–∞–±—ã–ª–º–∞–¥—ã. üèô"
#         }.get(lang, "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ–±—ä–µ–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
#         user_conversations[user_id].append({"role": "assistant", "content": msg})
#         return {"text": msg}

#     # --- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É–∂–µ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ ---
#     seen = shown_flats_cache[user_id]
#     new_flats = [f for f in flats if f.number not in seen][:4]
#     if not new_flats:
#         seen.clear()
#         new_flats = flats[:4]
#     for f in new_flats:
#         seen.add(f.number)

#     # --- –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ---
#     results = []
#     for f in new_flats:
#         text_base = (
#             f"üè† {f.type} ‚Ññ{f.number}\n"
#             f"‚Ä¢ –ö–æ–º–Ω–∞—Ç: {f.rooms}\n"
#             f"‚Ä¢ –≠—Ç–∞–∂: {f.stage}\n"
#             f"‚Ä¢ –ü–ª–æ—â–∞–¥—å: {f.sq_m} –º¬≤\n"
#             f"‚Ä¢ –¶–µ–Ω–∞: {f.price} $\n"
#             f"‚Ä¢ –ü–æ–¥—ä–µ–∑–¥: {f.lobby}\n"
#             "–° –≤–∞–º–∏ —Å–≤—è–∂–µ—Ç—Å—è –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π. üèô"
#         )
#         photo_val = normalize_url(f.plan.strip()) if getattr(f, "plan", None) else None
#         results.append({"text": text_base, "photo": photo_val})
#         user_conversations[user_id].append({"role": "assistant", "content": text_base})

#     last_filters_cache[user_id] = {}  # —Ñ–∏–ª—å—Ç—Ä—ã –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å –ø–æ–∑–∂–µ

#     return {"flats": results}


# # --- –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ ---
# def clear_user(user_id: int):
#     user_conversations[user_id].clear()
#     last_filters_cache.pop(user_id, None)
#     shown_flats_cache.pop(user_id, None)


# import os
# import logging
# import asyncio
# from collections import defaultdict
# from dotenv import load_dotenv
# from openai import OpenAI
# from db import Session, Flats as DBFlats
# from urllib.parse import urlsplit, urlunsplit, quote
# import json
# from datetime import datetime, timedelta

# # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
# load_dotenv()
# client = OpenAI(api_key=os.getenv("API_KEY"))
# logger = logging.getLogger(__name__)

# # === –ö—ç—à–∏ ===
# user_conversations = defaultdict(list)
# last_filters_cache = {}
# shown_flats_cache = defaultdict(set)
# gpt_cache = {}  # user_id: {"data": {...}, "expires": datetime}

# SUPPORTED_LANGS = {"ru", "uz", "en", "kk"}
# GPT_CACHE_TTL = timedelta(minutes=5)  # –∫—ç—à GPT –Ω–∞ 5 –º–∏–Ω—É—Ç

# # === –£—Ç–∏–ª–∏—Ç—ã ===
# def normalize_url(url: str) -> str:
#     try:
#         parts = urlsplit(url)
#         path = quote(parts.path, safe="/%") if parts.path else ""
#         query = quote(parts.query, safe="=&?") if parts.query else ""
#         return urlunsplit((parts.scheme, parts.netloc, path, query, parts.fragment))
#     except Exception:
#         return url

# def get_formatted_dialog(user_id: int) -> str:
#     msgs = user_conversations.get(user_id, [])
#     if not msgs:
#         return "‚Äî"
#     lines = []
#     for m in msgs:
#         prefix = "üë§" if m["role"] == "user" else "ü§ñ"
#         lines.append(f"{prefix} {m['content'].strip()}")
#     return "\n".join(lines)

# # === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
# async def ask_openai_sync(user_id: int, text: str):
#     text = text.strip()
#     if not text:
#         return {"text": "‚ùó –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"}

#     user_conversations[user_id].append({"role": "user", "content": text})

#     now = datetime.utcnow()
#     cached = gpt_cache.get(user_id)
#     if cached and cached["expires"] > now:
#         gpt_data = cached["data"]
#     else:
#         try:
#             gpt_prompt = f"""
# –¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏ –∏ JSON-–ø–∞—Ä—Å–∏–Ω–≥—É. 
# –°—Ç—Ä–æ–≥–æ –≤–æ–∑–≤—Ä–∞—â–∞–π JSON —Å —Ñ–∏–ª—å—Ç—Ä–∞–º–∏, –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.

# 1. –û–ø—Ä–µ–¥–µ–ª–∏ —è–∑—ã–∫ –∑–∞–ø—Ä–æ—Å–∞ (ru, en, uz, kk) -> "lang".
# 2. –û–ø—Ä–µ–¥–µ–ª–∏ —Ñ–∏–ª—å—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤:
#    - type: "–ö–≤–∞—Ä—Ç–∏—Ä–∞", "–°—Ç—É–¥–∏—è", "–ú–∞–≥–∞–∑–∏–Ω" –∏–ª–∏ null
#    - rooms: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç (int) –∏–ª–∏ null
#    - stage: —ç—Ç–∞–∂ (int) –∏–ª–∏ null
#    - price_max: –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ (int) –∏–ª–∏ null
#    - price_order: "min" –∏–ª–∏ "max" –∏–ª–∏ null

# –í–æ–∑–≤—Ä–∞—â–∞–π —Å—Ç—Ä–æ–≥–æ JSON:

# {{
#   "filters": {{
#     "type": "...",
#     "rooms": ...,
#     "stage": ...,
#     "price_max": ...,
#     "price_order": "..."
#   }},
#   "lang": "..."
# }}

# –ü—Ä–∏–º–µ—Ä: "2-–∫–æ–º–Ω–∞—Ç–Ω–∞—è –∫–≤–∞—Ä—Ç–∏—Ä–∞ –¥–æ 50000, –¥–µ—à–µ–≤–æ" -> {{
#   "filters": {{
#     "type": "–ö–≤–∞—Ä—Ç–∏—Ä–∞",
#     "rooms": 2,
#     "stage": null,
#     "price_max": 50000,
#     "price_order": "min"
#   }},
#   "lang": "ru"
# }}

# –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{text}"
# """
#             response = await client.chat.completions.acreate(
#                 model="gpt-5-nano",
#                 messages=[{"role": "user", "content": gpt_prompt}],
#             )
#             gpt_output = response.choices[0].message.content.strip()
#             gpt_data = json.loads(gpt_output)

#             # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–∏–ø–æ–≤
#             filters = gpt_data.get("filters", {})
#             filters["rooms"] = int(filters["rooms"]) if filters.get("rooms") is not None else None
#             filters["stage"] = int(filters["stage"]) if filters.get("stage") is not None else None
#             filters["price_max"] = int(filters["price_max"]) if filters.get("price_max") is not None else None
#             filters["type"] = filters.get("type") if filters.get("type") in {"–ö–≤–∞—Ä—Ç–∏—Ä–∞","–°—Ç—É–¥–∏—è","–ú–∞–≥–∞–∑–∏–Ω"} else None
#             filters["price_order"] = filters.get("price_order") if filters.get("price_order") in {"min","max"} else None
#             gpt_data["filters"] = filters
#             gpt_cache[user_id] = {"data": gpt_data, "expires": now + GPT_CACHE_TTL}
#         except Exception as e:
#             logger.error(f"–û—à–∏–±–∫–∞ GPT –∏–ª–∏ JSON: {e}")
#             gpt_data = {"filters": {}, "lang": "ru"}

#     filters = gpt_data.get("filters", {})
#     lang = gpt_data.get("lang", "ru")

#     if not any(filters.values()):
#         msg = {
#             "ru": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –ø–æ–∂–µ–ª–∞–Ω–∏–µ üí¨",
#             "uz": "Iltimos, kamida bitta talabni kiriting üí¨",
#             "en": "Please specify at least one preference üí¨",
#             "kk": "–ö–µ–º—ñ–Ω–¥–µ –±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–¥—ñ –∫”©—Ä—Å–µ—Ç—ñ“£—ñ–∑ üí¨"
#         }.get(lang, "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞.")
#         user_conversations[user_id].append({"role": "assistant", "content": msg})
#         return {"text": msg}

#     # --- SQLAlchemy –≤—ã–±–æ—Ä–∫–∞ ---
#     session = Session()
#     query = session.query(DBFlats)

#     if filters.get("type"):
#         query = query.filter(DBFlats.type == filters["type"])
#     if filters.get("rooms"):
#         query = query.filter(DBFlats.rooms == filters["rooms"])
#     if filters.get("stage"):
#         query = query.filter(DBFlats.stage == filters["stage"])
#     if filters.get("price_max"):
#         query = query.filter(DBFlats.price <= filters["price_max"])
#     if filters.get("price_order") == "min":
#         query = query.order_by(DBFlats.price.asc())
#     elif filters.get("price_order") == "max":
#         query = query.order_by(DBFlats.price.desc())

#     flats = query.limit(50).all()
#     session.close()

#     if not flats:
#         msg = {
#             "ru": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ–±—ä–µ–∫—Ç—ã —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. üèô",
#             "uz": "Afsuski, bunday parametrli obyektlar topilmadi. üèô",
#             "en": "Unfortunately, no properties match these parameters. üèô",
#             "kk": "”®–∫—ñ–Ω—ñ—à–∫–µ –æ—Ä–∞–π, –º“±–Ω–¥–∞–π –ø–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä–≥–µ —Å–∞–π –Ω—ã—Å–∞–Ω–¥–∞—Ä —Ç–∞–±—ã–ª–º–∞–¥—ã. üèô"
#         }.get(lang, "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ–±—ä–µ–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã.")
#         user_conversations[user_id].append({"role": "assistant", "content": msg})
#         return {"text": msg}

#     # --- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É–∂–µ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ ---
#     seen = shown_flats_cache[user_id]
#     new_flats = [f for f in flats if f.number not in seen][:4]
#     if not new_flats:
#         seen.clear()
#         new_flats = flats[:4]
#     for f in new_flats:
#         seen.add(f.number)

#     # --- –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç ---
#     results = []
#     for f in new_flats:
#         text_base = (
#             f"üè† {f.type} ‚Ññ{f.number}\n"
#             f"‚Ä¢ –ö–æ–º–Ω–∞—Ç: {f.rooms}\n"
#             f"‚Ä¢ –≠—Ç–∞–∂: {f.stage}\n"
#             f"‚Ä¢ –ü–ª–æ—â–∞–¥—å: {f.sq_m} –º¬≤\n"
#             f"‚Ä¢ –¶–µ–Ω–∞: {f.price} $\n"
#             f"‚Ä¢ –ü–æ–¥—ä–µ–∑–¥: {f.lobby}\n"
#             "–° –≤–∞–º–∏ —Å–≤—è–∂–µ—Ç—Å—è –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π. üèô"
#         )
#         photo_val = normalize_url(f.plan.strip()) if getattr(f, "plan", None) else None
#         results.append({"text": text_base, "photo": photo_val})
#         user_conversations[user_id].append({"role": "assistant", "content": text_base})

#     last_filters_cache[user_id] = filters
#     return {"flats": results}

# # --- –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ ---
# def clear_user(user_id: int):
#     user_conversations[user_id].clear()
#     last_filters_cache.pop(user_id, None)
#     shown_flats_cache.pop(user_id, None)
#     gpt_cache.pop(user_id, None)

# import os
# import json
# import logging
# import re
# import asyncio
# from collections import defaultdict
# from urllib.parse import urlsplit, urlunsplit, quote
# from dotenv import load_dotenv
# from aiogram import Bot
# from openai import OpenAI 
# from db import Session, Flats as DBFlats

# # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
# load_dotenv()
# client = OpenAI(api_key=os.getenv("API_KEY"))
# logger = logging.getLogger(__name__)

# # === –ö—ç—à–∏ ===
# user_conversations = defaultdict(list)
# last_filters_cache = {}
# shown_flats_cache = defaultdict(set)
# SUPPORTED_LANGS = {"ru", "uz", "en", "kk"}


# # === –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–∞—Ä—Å–µ—Ä ===
# def fallback_parse_filters(text: str) -> dict:
#     filters = {}

#     if match := re.search(r'(\d+)\s*–∫–æ–º–Ω–∞—Ç', text, re.IGNORECASE):
#         filters["rooms"] = int(match.group(1))
#     if match := re.search(r'(\d+)\s*(?:—ç—Ç–∞–∂|—ç—Ç–∞–∂–µ)', text, re.IGNORECASE):
#         filters["stage"] = int(match.group(1))
#     if match := re.search(r'(\d+[.,]?\d*)\s*(?:\$|–¥–æ–ª–ª–∞—Ä|—Ç—ã—Å)', text, re.IGNORECASE):
#         filters["price_max"] = float(match.group(1).replace(',', '.'))
#     if '–º–∞–≥–∞–∑–∏–Ω' in text.lower():
#         filters["type"] = "–ú–∞–≥–∞–∑–∏–Ω"
#     elif '—Å—Ç—É–¥' in text.lower():
#         filters["type"] = "–°—Ç—É–¥–∏—è"
#     else:
#         filters["type"] = "–ö–≤–∞—Ä—Ç–∏—Ä–∞"

#     return filters

# # === –£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è URL ===
# def normalize_url(url: str) -> str:
#     try:
#         parts = urlsplit(url)
#         path = quote(parts.path, safe="/%") if parts.path else ""
#         query = quote(parts.query, safe="=&?") if parts.query else ""
#         return urlunsplit((parts.scheme, parts.netloc, path, query, parts.fragment))
#     except Exception:
#         return url


# # === –ü–µ—á–∞—Ç–∞–µ—Ç... ===
# async def show_typing(bot: Bot, chat_id: int, duration: int = 5):
#     try:
#         end_time = asyncio.get_event_loop().time() + duration
#         while asyncio.get_event_loop().time() < end_time:
#             await bot.send_chat_action(chat_id, "typing")
#             await asyncio.sleep(4)
#     except Exception:
#         pass


# # === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ ===
# async def detect_language(text: str) -> str:
#     try:
#         prompt = f"""
# Detect the language of this text and respond ONLY with one of:
# ru, en, uz, kk.
# Text: "{text}"
# """
#         resp = client.responses.create(
#             model="gpt-5-nano",
#             instructions=[prompt],
#             input=text,
#         )
#         print('\n \n \n',"GPT LANG RAW:", resp , '\n \n \n')
#         lang = resp.output_text.strip().lower()
#         return lang if lang in SUPPORTED_LANGS else "ru"
#     except Exception:
#         return "ru"


# # === –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ ===
# async def extract_filters_with_gpt(text: str) -> dict:
#     try:
#         prompt = f"""
# You are a real estate filter extractor.
# Extract parameters from the user's request and return ONLY valid JSON.

# Fields:
# - type: "–ö–≤–∞—Ä—Ç–∏—Ä–∞" | "–°—Ç—É–¥–∏—è" | "–ú–∞–≥–∞–∑–∏–Ω"
# - rooms: integer
# - stage: integer
# - price_max: integer
# - price_order: "min" | "max"

# User request: "{text}"

# Respond ONLY with JSON, no explanation.
# Example:
# {{"type": "–ö–≤–∞—Ä—Ç–∏—Ä–∞", "rooms": 2, "price_max": 50000}}
# """
#         resp = client.responses.create(
#             model="gpt-5-nano",
#             instructions=[prompt],
#             input=text,
#         )
#         raw = resp.output_text.strip()
#         print('\n \n \n',"GPT RAW:", raw , '\n \n \n')
#         data = json.loads(raw)
#         if isinstance(data, dict):
#             logger.info(f"‚úÖ GPT parsed filters: {data}")
#             return data
#         return {}
#     except Exception as e:
#         logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ GPT: {e}")
#         return {}


# # === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
# async def ask_openai_sync(user_id: int, text: str, bot: Bot = None, chat_id: int = None):
#     print("\n\nUSER MESSAGE:", text, "\n\n")
#     text = text.strip()
#     if not text:
#         return {"text": "‚ùó –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"}

#     if bot and chat_id:
#         asyncio.create_task(show_typing(bot, chat_id, duration=5))

#     # --- –Ø–∑—ã–∫
#     lang = await detect_language(text)
#     user_conversations[user_id].append(text)

#     # --- –§–∏–ª—å—Ç—Ä—ã GPT
#     filters = await extract_filters_with_gpt(text)
#     if not filters:
#         filters = fallback_parse_filters(text)
#         if filters:
#             logger.info(f"‚öôÔ∏è GPT –Ω–µ –≤–µ—Ä–Ω—É–ª —Ñ–∏–ª—å—Ç—Ä—ã, fallback: {filters}")

#     if not filters:
#         msg = {
#             "ru": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –ø–æ–∂–µ–ª–∞–Ω–∏–µ üí¨",
#             "uz": "Iltimos, kamida bitta talabni kiriting üí¨",
#             "en": "Please specify at least one preference üí¨",
#             "kk": "–ö–µ–º –¥–µ–≥–µ–Ω–¥–µ –±—ñ—Ä “õ–∞–ª–∞—É—ã“£—ã–∑–¥—ã –∫”©—Ä—Å–µ—Ç—ñ“£—ñ–∑ üí¨",
#         }[lang]
#         return {"text": msg}

#     last_filters_cache[user_id] = filters
#     shown_flats_cache[user_id].clear()

#     if bot and chat_id:
#         asyncio.create_task(show_typing(bot, chat_id, duration=5))

#     # --- –ü–æ–∏—Å–∫ –≤ –ë–î
#     session = Session()
#     query = session.query(DBFlats)

#     if filters.get("type"):
#         query = query.filter(DBFlats.type == filters["type"])
#     if filters.get("rooms"):
#         query = query.filter(DBFlats.rooms == filters["rooms"])
#     if filters.get("stage"):
#         query = query.filter(DBFlats.stage == filters["stage"])
#     if filters.get("price_max"):
#         query = query.filter(DBFlats.price <= filters["price_max"])
#     if filters.get("price_order") == "min":
#         query = query.order_by(DBFlats.price.asc())
#     elif filters.get("price_order") == "max":
#         query = query.order_by(DBFlats.price.desc())

#     flats = query.filter(DBFlats.status == "–°–≤–æ–±–æ–¥–Ω–æ").all()
#     session.close()

#     if not flats:
#         msg = {
#             "ru": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ–±—ä–µ–∫—Ç—ã —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. üèô",
#             "uz": "Afsuski, bunday parametrli obyektlar topilmadi. üèô",
#             "en": "Unfortunately, no properties match these parameters. üèô",
#             "kk": "”®–∫—ñ–Ω—ñ—à–∫–µ –æ—Ä–∞–π, –º“±–Ω–¥–∞–π –ø–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä–º–µ–Ω –Ω—ã—Å–∞–Ω–¥–∞—Ä —Ç–∞–±—ã–ª–º–∞–¥—ã. üèô",
#         }[lang]
#         return {"text": msg}

#     seen = shown_flats_cache[user_id]
#     new_flats = [f for f in flats if f.number not in seen][:4]
#     if not new_flats:
#         seen.clear()
#         new_flats = flats[:4]
#     for f in new_flats:
#         seen.add(f.number)

#     results = []
#     for f in new_flats:
#         text_base = (
#             f"üè† {f.type} ‚Ññ{f.number}\n"
#             f"‚Ä¢ –ö–æ–º–Ω–∞—Ç: {f.rooms}\n"
#             f"‚Ä¢ –≠—Ç–∞–∂: {f.stage}\n"
#             f"‚Ä¢ –ü–ª–æ—â–∞–¥—å: {f.sq_m} –º¬≤\n"
#             f"‚Ä¢ –¶–µ–Ω–∞: {f.price} $\n"
#             f"‚Ä¢ –ü–æ–¥—ä–µ–∑–¥: {f.lobby}\n"
#             f"{f.description}\n\n"
#             "–° –≤–∞–º–∏ —Å–≤—è–∂–µ—Ç—Å—è –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π. üèô"
#         )

#         # –ü–µ—Ä–µ–≤–æ–¥ –æ–ø–∏—Å–∞–Ω–∏—è, –µ—Å–ª–∏ —è–∑—ã–∫ –Ω–µ —Ä—É—Å—Å–∫–∏–π
#         if lang != "ru":
#             try:
#                 translation = client.responses.create(
#                 model="gpt-5-nano",
#             instructions=[f"–ü–µ—Ä–µ–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç –Ω–∞ {lang}, –Ω–µ –∏–∑–º–µ–Ω—è—è —á–∏—Å–ª–∞ –∏ –Ω–∞–∑–≤–∞–Ω–∏—è –ñ–ö.",text_base],
#             input=text,
#                 )
#                 text_base = translation.output_text.strip()
#             except Exception:
#                 pass

#         photo_val = normalize_url(f.plan.strip()) if getattr(f, "plan", None) else None
#         results.append({"text": text_base, "photo": photo_val})

#     return {"flats": results}


# # === –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ ===
# def clear_user(user_id: int):
#     user_conversations[user_id].clear()
#     last_filters_cache.pop(user_id, None)
#     shown_flats_cache.pop(user_id, None)

# import os
# import json
# import logging
# import re
# import asyncio
# from collections import defaultdict
# from urllib.parse import urlsplit, urlunsplit, quote
# from dotenv import load_dotenv
# from aiogram import Bot
# from openai import OpenAI
# from db import Session, Flats as DBFlats

# # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
# load_dotenv()
# client = OpenAI(api_key=os.getenv("API_KEY"))
# logger = logging.getLogger(__name__)

# # === –ö—ç—à–∏ ===
# user_conversations = defaultdict(list)
# last_filters_cache = {}
# shown_flats_cache = defaultdict(set)
# SUPPORTED_LANGS = {"ru", "uz", "en", "kk"}


# # === –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–∞—Ä—Å–µ—Ä ===
# def fallback_parse_filters(text: str) -> dict:
#     filters = {}

#     if match := re.search(r'(\d+)\s*–∫–æ–º–Ω–∞—Ç', text, re.IGNORECASE):
#         filters["rooms"] = int(match.group(1))
#     if match := re.search(r'(\d+)\s*(?:—ç—Ç–∞–∂|—ç—Ç–∞–∂–µ)', text, re.IGNORECASE):
#         filters["stage"] = int(match.group(1))
#     if match := re.search(r'(\d+[.,]?\d*)\s*(?:\$|–¥–æ–ª–ª–∞—Ä|—Ç—ã—Å)', text, re.IGNORECASE):
#         filters["price_max"] = float(match.group(1).replace(',', '.'))
#     if '–º–∞–≥–∞–∑–∏–Ω' in text.lower():
#         filters["type"] = "–ú–∞–≥–∞–∑–∏–Ω"
#     elif '—Å—Ç—É–¥' in text.lower():
#         filters["type"] = "–°—Ç—É–¥–∏—è"
#     else:
#         filters["type"] = "–ö–≤–∞—Ä—Ç–∏—Ä–∞"

#     return filters


# # === –£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è URL ===
# def normalize_url(url: str) -> str:
#     try:
#         parts = urlsplit(url)
#         path = quote(parts.path, safe="/%") if parts.path else ""
#         query = quote(parts.query, safe="=&?") if parts.query else ""
#         return urlunsplit((parts.scheme, parts.netloc, path, query, parts.fragment))
#     except Exception:
#         return url


# # === –ü–µ—á–∞—Ç–∞–µ—Ç... ===
# async def show_typing(bot: Bot, chat_id: int, duration: int = 5):
#     try:
#         end_time = asyncio.get_event_loop().time() + duration
#         while asyncio.get_event_loop().time() < end_time:
#             await bot.send_chat_action(chat_id, "typing")
#             await asyncio.sleep(4)
#     except Exception:
#         pass


# # === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞ ===
# async def detect_language(text: str) -> str:
#     try:
#         messages = [
#             {"role": "system", "content": "Respond ONLY with ru, en, uz, or kk."},
#             {"role": "user", "content": f"Detect the language of this text: {text}"}
#         ]
#         resp = client.chat.completions.create(
#             model="gpt-4.1-mini",
#             messages=messages,
#             max_tokens=5
#         )
#         lang = resp.choices[0].message.content.strip().lower()
#         return lang if lang in SUPPORTED_LANGS else "ru"
#     except Exception:
#         return "ru"


# # === –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ ===
# async def extract_filters_with_gpt(text: str) -> dict:
#     try:
#         messages = [
#             {"role": "system", "content": (
#                 "You are a real estate filter extractor. "
#                 "Extract parameters from the user's request and return ONLY valid JSON with these fields: "
#                 "{type, rooms, stage, price_max, price_order}."
#             )},
#             {"role": "user", "content": text}
#         ]
#         resp = client.chat.completions.create(
#             model="gpt-4.1-mini",
#             messages=messages,
#             max_tokens=200
#         )
#         raw = resp.choices[0].message.content.strip()
#         print("\n\nGPT RAW:", raw, "\n\n")
#         data = json.loads(raw)
#         if isinstance(data, dict):
#             logger.info(f"‚úÖ GPT parsed filters: {data}")
#             return data
#         return {}
#     except Exception as e:
#         logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ GPT: {e}")
#         return {}


# # === –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ===
# async def ask_openai_sync(user_id: int, text: str, bot: Bot = None, chat_id: int = None):
#     print("\n\nUSER MESSAGE:", text, "\n\n")
#     text = text.strip()
#     if not text:
#         return {"text": "‚ùó –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"}

#     if bot and chat_id:
#         asyncio.create_task(show_typing(bot, chat_id, duration=5))

#     lang = await detect_language(text)
#     user_conversations[user_id].append(text)

#     filters = await extract_filters_with_gpt(text)
#     if not filters:
#         filters = fallback_parse_filters(text)
#         if filters:
#             logger.info(f"‚öôÔ∏è GPT –Ω–µ –≤–µ—Ä–Ω—É–ª —Ñ–∏–ª—å—Ç—Ä—ã, fallback: {filters}")

#     if not filters:
#         msg = {
#             "ru": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –ø–æ–∂–µ–ª–∞–Ω–∏–µ üí¨",
#             "uz": "Iltimos, kamida bitta talabni kiriting üí¨",
#             "en": "Please specify at least one preference üí¨",
#             "kk": "–ö–µ–º –¥–µ–≥–µ–Ω–¥–µ –±—ñ—Ä “õ–∞–ª–∞—É—ã“£—ã–∑–¥—ã –∫”©—Ä—Å–µ—Ç—ñ“£—ñ–∑ üí¨",
#         }[lang]
#         return {"text": msg}

#     last_filters_cache[user_id] = filters
#     shown_flats_cache[user_id].clear()

#     if bot and chat_id:
#         asyncio.create_task(show_typing(bot, chat_id, duration=5))

#     # === –ü–æ–∏—Å–∫ –≤ –ë–î ===
#     session = Session()
#     query = session.query(DBFlats)

#     if filters.get("type"):
#         query = query.filter(DBFlats.type == filters["type"])
#     if filters.get("rooms"):
#         query = query.filter(DBFlats.rooms == filters["rooms"])
#     if filters.get("stage"):
#         query = query.filter(DBFlats.stage == filters["stage"])
#     if filters.get("price_max"):
#         query = query.filter(DBFlats.price <= filters["price_max"])
#     if filters.get("price_order") == "min":
#         query = query.order_by(DBFlats.price.asc())
#     elif filters.get("price_order") == "max":
#         query = query.order_by(DBFlats.price.desc())

#     flats = query.filter(DBFlats.status == "–°–≤–æ–±–æ–¥–Ω–æ").all()
#     session.close()

#     if not flats:
#         msg = {
#             "ru": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ–±—ä–µ–∫—Ç—ã —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. üèô",
#             "uz": "Afsuski, bunday parametrli obyektlar topilmadi. üèô",
#             "en": "Unfortunately, no properties match these parameters. üèô",
#             "kk": "”®–∫—ñ–Ω—ñ—à–∫–µ –æ—Ä–∞–π, –º“±–Ω–¥–∞–π –ø–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä–º–µ–Ω –Ω—ã—Å–∞–Ω–¥–∞—Ä —Ç–∞–±—ã–ª–º–∞–¥—ã. üèô",
#         }[lang]
#         return {"text": msg}

#     seen = shown_flats_cache[user_id]
#     new_flats = [f for f in flats if f.number not in seen][:4]
#     if not new_flats:
#         seen.clear()
#         new_flats = flats[:4]
#     for f in new_flats:
#         seen.add(f.number)

#     results = []
#     for f in new_flats:
#         text_base = (
#             f"üè† {f.type} ‚Ññ{f.number}\n"
#             f"‚Ä¢ –ö–æ–º–Ω–∞—Ç: {f.rooms}\n"
#             f"‚Ä¢ –≠—Ç–∞–∂: {f.stage}\n"
#             f"‚Ä¢ –ü–ª–æ—â–∞–¥—å: {f.sq_m} –º¬≤\n"
#             f"‚Ä¢ –¶–µ–Ω–∞: {f.price} $\n"
#             f"‚Ä¢ –ü–æ–¥—ä–µ–∑–¥: {f.lobby}\n"
#             f"{f.description}\n\n"
#             "–° –≤–∞–º–∏ —Å–≤—è–∂–µ—Ç—Å—è –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π. üèô"
#         )

#         if lang != "ru":
#             try:
#                 messages = [
#                     {"role": "system", "content": f"Translate the text to {lang}, keep numbers and names unchanged."},
#                     {"role": "user", "content": text_base}
#                 ]
#                 translation = client.chat.completions.create(
#                     model="gpt-4.1-mini",
#                     messages=messages
#                 )
#                 text_base = translation.choices[0].message.content.strip()
#             except Exception:
#                 pass

#         photo_val = normalize_url(f.plan.strip()) if getattr(f, "plan", None) else None
#         results.append({"text": text_base, "photo": photo_val})

#     return {"flats": results}


# # === –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ ===
# def clear_user(user_id: int):
#     user_conversations[user_id].clear()
#     last_filters_cache.pop(user_id, None)
#     shown_flats_cache.pop(user_id, None)


import os
import json
import logging
import re
import asyncio
from collections import defaultdict
from urllib.parse import urlsplit, urlunsplit, quote
from dotenv import load_dotenv
from aiogram import Bot
from openai import OpenAI
from db import Session, Flats as DBFlats

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ===
load_dotenv()
client = OpenAI(api_key=os.getenv("API_KEY"))
logger = logging.getLogger(__name__)

# === –ö—ç—à–∏ ===
user_conversations = defaultdict(list)
last_filters_cache = {}
shown_flats_cache = defaultdict(set)
SUPPORTED_LANGS = {"ru", "uz", "en", "kk"}


# === –†–ï–ó–ï–†–í–ù–´–ô –ü–ê–†–°–ï–† (fallback) ===
def fallback_parse_filters(text: str) -> dict:
    filters = {}

    # 1‚Äì5 –∫–æ–º–Ω–∞—Ç
    if match := re.search(r'(\d+)\s*[- ]?\s*–∫–æ–º–Ω–∞—Ç', text, re.IGNORECASE):
        filters["rooms"] = int(match.group(1))

    # —ç—Ç–∞–∂
    if match := re.search(r'(\d+)\s*(?:—ç—Ç–∞–∂|—ç—Ç–∞–∂–µ)', text, re.IGNORECASE):
        filters["stage"] = int(match.group(1))

    # —Ü–µ–Ω–∞ –¥–æ / –º–∞–∫—Å–∏–º—É–º
    if match := re.search(r'(\d+[.,]?\d*)\s*(?:\$|–¥–æ–ª–ª–∞—Ä|—Ç—ã—Å|—Ç—ã—Å—è—á)', text, re.IGNORECASE):
        price = float(match.group(1).replace(',', '.'))
        if '—Ç—ã—Å' in text.lower():
            price *= 1000
        filters["price_max"] = int(price)

    # —Ç–∏–ø
    low = text.lower()
    if '–º–∞–≥–∞–∑–∏–Ω' in low:
        filters["type"] = "–ú–∞–≥–∞–∑–∏–Ω"
    elif '—Å—Ç—É–¥' in low or '1 –∫–æ–º–Ω–∞—Ç' in low:
        filters["type"] = "–°—Ç—É–¥–∏—è"
    else:
        filters["type"] = "–ö–≤–∞—Ä—Ç–∏—Ä–∞"

    return filters


# === –£–¢–ò–õ–ò–¢–ê –î–õ–Ø URL ===
def normalize_url(url: str) -> str:
    try:
        parts = urlsplit(url)
        path = quote(parts.path, safe="/%") if parts.path else ""
        query = quote(parts.query, safe="=&?") if parts.query else ""
        return urlunsplit((parts.scheme, parts.netloc, path, query, parts.fragment))
    except Exception:
        return url


# === "–ü–ï–ß–ê–¢–ê–ï–¢..." ===
async def show_typing(bot: Bot, chat_id: int, duration: int = 5):
    try:
        end_time = asyncio.get_event_loop().time() + duration
        while asyncio.get_event_loop().time() < end_time:
            await bot.send_chat_action(chat_id, "typing")
            await asyncio.sleep(4)
    except Exception:
        pass


# === –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –Ø–ó–´–ö–ê ===
async def detect_language(text: str) -> str:
    try:
        resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {"role": "system", "content": "Respond ONLY with one code: ru, en, uz, kk."},
                {"role": "user", "content": text},
            ],
            temperature=0,
            max_tokens=5,
        )
        lang = resp.choices[0].message.content.strip().lower()
        return lang if lang in SUPPORTED_LANGS else "ru"
    except Exception:
        return "ru"


# === GPT-–ü–ê–†–°–ï–† –§–ò–õ–¨–¢–†–û–í ===
async def extract_filters_with_gpt(text: str) -> dict:
    """
    GPT –ø–∞—Ä—Å–∏—Ç —Ñ–∏–ª—å—Ç—Ä—ã –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –≤–æ–∑–≤—Ä–∞—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ JSON.
    """
    try:
        messages = [
            {
                "role": "system",
                "content": (
                    "–¢—ã –ø–∞—Ä—Å–µ—Ä —Ñ–∏–ª—å—Ç—Ä–æ–≤ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏. "
                    "–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ JSON –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤. "
                    "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç ‚Äî –≤–µ—Ä–Ω–∏ '{}'. "
                    "–ü–æ–ª—è: type (–ö–≤–∞—Ä—Ç–∏—Ä–∞|–°—Ç—É–¥–∏—è|–ú–∞–≥–∞–∑–∏–Ω), rooms (int), stage (int), "
                    "price_max (int), price_order (min|max)."
                ),
            },
            {"role": "user", "content": text},
        ]

        resp = client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=messages,
            temperature=0.1,
            max_tokens=200,
        )

        raw = resp.choices[0].message.content.strip()
        print("\n[GPT RAW FILTERS]:", raw, "\n")

        # —É–±–∏—Ä–∞–µ–º markdown-–º—É—Å–æ—Ä ```json ```
        cleaned = re.sub(r"```(?:json)?|```", "", raw).strip()

        data = json.loads(cleaned)
        if not isinstance(data, dict):
            raise ValueError("GPT –æ—Ç–≤–µ—Ç–∏–ª –Ω–µ JSON")

        logger.info(f"‚úÖ GPT parsed filters: {data}")
        return data

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ GPT: {e}")
        filters = fallback_parse_filters(text)
        logger.info(f"üîÑ –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback: {filters}")
        return filters


# === –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ===
async def ask_openai_sync(user_id: int, text: str, bot: Bot = None, chat_id: int = None):
    print(f"\n=== USER MESSAGE ===\n{text}\n====================\n")
    text = text.strip()
    if not text:
        return {"text": "‚ùó –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å"}

    if bot and chat_id:
        asyncio.create_task(show_typing(bot, chat_id, duration=5))

    # —è–∑—ã–∫
    lang = await detect_language(text)
    user_conversations[user_id].append(text)

    # —Ñ–∏–ª—å—Ç—Ä—ã
    filters = await extract_filters_with_gpt(text)
    if not filters:
        filters = fallback_parse_filters(text)

    if not filters:
        msg = {
            "ru": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –ø–æ–∂–µ–ª–∞–Ω–∏–µ üí¨",
            "uz": "Iltimos, kamida bitta talabni kiriting üí¨",
            "en": "Please specify at least one preference üí¨",
            "kk": "–ö–µ–º –¥–µ–≥–µ–Ω–¥–µ –±—ñ—Ä “õ–∞–ª–∞—É—ã“£—ã–∑–¥—ã –∫”©—Ä—Å–µ—Ç—ñ“£—ñ–∑ üí¨",
        }[lang]
        return {"text": msg}

    last_filters_cache[user_id] = filters
    shown_flats_cache[user_id].clear()

    if bot and chat_id:
        asyncio.create_task(show_typing(bot, chat_id, duration=5))

    # === –ü–æ–∏—Å–∫ –≤ –ë–î ===
    session = Session()
    query = session.query(DBFlats)

    if filters.get("type"):
        query = query.filter(DBFlats.type == filters["type"])
    if filters.get("rooms"):
        query = query.filter(DBFlats.rooms == filters["rooms"])
    if filters.get("stage"):
        query = query.filter(DBFlats.stage == filters["stage"])
    if filters.get("price_max"):
        query = query.filter(DBFlats.price <= filters["price_max"])
    if filters.get("price_order") == "min":
        query = query.order_by(DBFlats.price.asc())
    elif filters.get("price_order") == "max":
        query = query.order_by(DBFlats.price.desc())

    flats = query.filter(DBFlats.status == "–°–≤–æ–±–æ–¥–Ω–æ").all()
    session.close()

    if not flats:
        msg = {
            "ru": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –æ–±—ä–µ–∫—Ç—ã —Å —Ç–∞–∫–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. üèô",
            "uz": "Afsuski, bunday parametrli obyektlar topilmadi. üèô",
            "en": "Unfortunately, no properties match these parameters. üèô",
            "kk": "”®–∫—ñ–Ω—ñ—à–∫–µ –æ—Ä–∞–π, –º“±–Ω–¥–∞–π –ø–∞—Ä–∞–º–µ—Ç—Ä–ª–µ—Ä–º–µ–Ω –Ω—ã—Å–∞–Ω–¥–∞—Ä —Ç–∞–±—ã–ª–º–∞–¥—ã. üèô",
        }[lang]
        return {"text": msg}

    # === –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –ø–æ–∫–∞–∑–∞–Ω–Ω—ã–µ ===
    seen = shown_flats_cache[user_id]
    new_flats = [f for f in flats if f.number not in seen][:4]
    if not new_flats:
        seen.clear()
        new_flats = flats[:4]
    for f in new_flats:
        seen.add(f.number)

    # === –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã–≤–æ–¥ ===
    results = []
    for f in new_flats:
        text_base = (
            f"üè† {f.type} ‚Ññ{f.number}\n"
            f"‚Ä¢ –ö–æ–º–Ω–∞—Ç: {f.rooms}\n"
            f"‚Ä¢ –≠—Ç–∞–∂: {f.stage}\n"
            f"‚Ä¢ –ü–ª–æ—â–∞–¥—å: {f.sq_m} –º¬≤\n"
            f"‚Ä¢ –¶–µ–Ω–∞: {f.price} $\n"
            f"‚Ä¢ –ü–æ–¥—ä–µ–∑–¥: {f.lobby}\n\n"
            f"{f.description}\n\n"
            "–° –≤–∞–º–∏ —Å–≤—è–∂–µ—Ç—Å—è –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π. üèô"
        )

        # –ø–µ—Ä–µ–≤–æ–¥, –µ—Å–ª–∏ —è–∑—ã–∫ –Ω–µ —Ä—É—Å—Å–∫–∏–π
        if lang != "ru":
            try:
                translation = client.chat.completions.create(
                    model="gpt-4.1-mini",
                    messages=[
                        {
                            "role": "system",
                            "content": f"Translate to {lang}, but keep numbers and building names unchanged.",
                        },
                        {"role": "user", "content": text_base},
                    ],
                    temperature=0,
                )
                text_base = translation.choices[0].message.content.strip()
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")

        photo_val = normalize_url(f.plan.strip()) if getattr(f, "plan", None) else None
        results.append({"text": text_base, "photo": photo_val})

    return {"flats": results}


# === –û—á–∏—Å—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ ===
def clear_user(user_id: int):
    user_conversations[user_id].clear()
    last_filters_cache.pop(user_id, None)
    shown_flats_cache.pop(user_id, None)
